{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import the Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load the Face Detector Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serialized face detector model\n",
    "prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
    "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the Pre-trained CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for face mask detection\n",
    "maskClassifier = load_model(\"cnn-model.h5\")\n",
    "\n",
    "# Define model constants \n",
    "img_size = 224\n",
    "channels = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define the functions**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detect_face` function is used for detecting the faces captured by the webcam. This function returns a tuple containing a list of extracted faces and a list of the faces' positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (img_size, img_size), (104.0, 177.0, 123.0))\n",
    "\n",
    "    # Pass the blob through the network and obtain the face detections\n",
    "    faceNet.setInput(blob)\n",
    "    detections = faceNet.forward()\n",
    "\n",
    "    # Initialize the list of faces and their corresponding locations\n",
    "    faces = []\n",
    "    locs = []\n",
    "\n",
    "    # Loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # Extract the confidence (i.e., probability) associated with the detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        \n",
    "        # Filter out weak detections by setting a minimum confidence\n",
    "        if confidence > 0.5:\n",
    "            # Compute the (x, y)-coordinates of the bounding box for the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Ensure the bounding boxes fall within the dimensions of the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "            # Extract the face's region of interest (ROI)\n",
    "            faceROI = frame[startY:endY, startX:endX]\n",
    "            # Convert the extracted face from BGR to RGB channel\n",
    "            faceROI = cv2.cvtColor(faceROI, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Ddd the face and bounding boxes to their respective lists\n",
    "            faces.append(faceROI)\n",
    "            locs.append((startX, startY, endX, endY))\n",
    "            \n",
    "    return (faces, locs)\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function is used to make predictions using the CNN model. This function first preprocesses the image input to fit the required image size. Afterwards, using the imported model, the predictions are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(face):\n",
    "    # Preprocess the image\n",
    "    face = cv2.resize(face, (img_size, img_size))\n",
    "    face = img_to_array(face)\n",
    "    face = face.reshape(1, img_size, img_size, channels)\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = maskClassifier(face)\n",
    "    return prediction[0].numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_ARFilter` function is used to add the Augmented Reality (AR) Filter over the faces of the people captured by the webcam. The filter changes color depending on the prediction and a label is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ARFilter(frame, faces, locs):\n",
    "    preds = []\n",
    "\n",
    "    # Only make predictions when there are faces detected\n",
    "    if len(faces) > 0:\n",
    "        for face in faces:\n",
    "            preds.append(predict(face))\n",
    "    \n",
    "    # Only add filter when there are faces detected and predictions made\n",
    "    if len(locs) > 0 and len(preds) > 0:\n",
    "        for (box, pred) in zip(locs, preds):\n",
    "            # Unpack the bounding box and predictions\n",
    "            (startX, startY, endX, endY) = box\n",
    "            (withmask, withoutmask) = pred\n",
    "\n",
    "            # Define the label and color of the filter\n",
    "            (label, color) = (\n",
    "                (f\"Mask: {np.round(withmask * 100.00, 2)}%\", (0, 255, 0))\n",
    "                if withmask > withoutmask\n",
    "                else (f\"No Mask: {np.round(withoutmask * 100.00, 2)}%\", (0, 0, 255), )\n",
    "            )\n",
    "\n",
    "            # Add filter (text label and bounding box) using OpenCV\n",
    "            cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Open the Webcam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a video capture object to read input from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error opening webcam\")\n",
    "\n",
    "# Set the size of the video window\n",
    "cap.set(3, 640)  # Width\n",
    "cap.set(4, 480)  # Height"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detect and Classify Face Mask Use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam has been closed\n"
     ]
    }
   ],
   "source": [
    "# Start reading the webcam stream and detect face masks in the video\n",
    "while True:\n",
    "    stream_ok, frame = cap.read()\n",
    "    # Check if the webcam stream is available\n",
    "    if not stream_ok:\n",
    "        print(\"Error reading webcam\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Detect faces and facemasks in the video frame using detect_face function\n",
    "    (faces, locs) = detect_face(frame)\n",
    "\n",
    "    # Add AR filter to the video frame based on the predictions\n",
    "    add_ARFilter(frame, faces, locs)\n",
    "    \n",
    "    # Displaying the current video frame\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    # Press 'q' to exit the video stream\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        print(\"Webcam has been closed\")\n",
    "        break\n",
    "\n",
    "# Video Stream cleanup and capture release\n",
    "cap.release()  \n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58837b1b657ea91009af8409fc244ae3b5ccf93ea980d6fb6b80adc5f697f4cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
